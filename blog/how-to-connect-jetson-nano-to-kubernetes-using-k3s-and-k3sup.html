<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:creator" content="@Icetekio"/><meta property="og:locale" content="en_EN"/><meta property="og:site_name" content="Icetek | Hot technologies served cool"/><title>Icetek | How To Connect Jetson Nano To Kubernetes Using K3s And K3sup</title><meta name="robots" content="index,follow"/><meta name="description" content="In this article I will show how to connect a Jetson Nano Developer board to the Kubernetes cluster to act as a GPU node."/><meta property="og:title" content="How To Connect Jetson Nano To Kubernetes Using K3s And K3sup"/><meta property="og:description" content="In this article I will show how to connect a Jetson Nano Developer board to the Kubernetes cluster to act as a GPU node."/><meta property="og:url" content="https://icetek.io/blog/how-to-connect-jetson-nano-to-kubernetes-using-k3s-and-k3sup"/><meta property="og:type" content="website"/><meta property="og:image" content="/blog-images/jetson-header-img.jpeg"/><meta property="og:image:alt" content="How To Connect Jetson Nano To Kubernetes Using K3s And K3sup"/><meta name="next-head-count" content="15"/><meta charSet="utf-8"/><meta name="theme-color" content="#53E7FF"/><link rel="canonical" href="https://icetek.io/"/><link rel="apple-touch-icon" sizes="57x57" href="/images/favicon/apple-icon-57x57.png"/><link rel="apple-touch-icon" sizes="60x60" href="/images/favicon/apple-icon-60x60.png"/><link rel="apple-touch-icon" sizes="72x72" href="/images/favicon/apple-icon-72x72.png"/><link rel="apple-touch-icon" sizes="76x76" href="/images/favicon/apple-icon-76x76.png"/><link rel="apple-touch-icon" sizes="114x114" href="/images/favicon/apple-icon-114x114.png"/><link rel="apple-touch-icon" sizes="120x120" href="/images/favicon/apple-icon-120x120.png"/><link rel="apple-touch-icon" sizes="144x144" href="/images/favicon/apple-icon-144x144.png"/><link rel="apple-touch-icon" sizes="152x152" href="/images/favicon/apple-icon-152x152.png"/><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon/apple-icon-180x180.png"/><link rel="icon" type="image/png" sizes="192x192" href="/images/favicon/android-icon-192x192.png"/><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/images/favicon/favicon-96x96.png"/><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon/favicon-16x16.png"/><link rel="manifest" href="/manifest.json"/><link rel="preload" href="/fonts/iceGX.ttf" as="font" type="font/ttf" crossorigin="anonymous"/><link rel="preload" href="/fonts/Icetek_newGX.ttf" as="font" type="font/ttf" crossorigin="anonymous"/><link rel="preload" href="/fonts/3B1CDA_E_0.woff2" as="font" type="font/woff2" crossorigin="anonymous"/><link rel="preload" href="/fonts/3B1CDA_4_0.woff2" as="font" type="font/woff2" crossorigin="anonymous"/><link rel="preload" href="/fonts/3B1CDA_1_0.woff2" as="font" type="font/woff2" crossorigin="anonymous"/><link rel="preload" href="https://icetek.io/_next/static/css/f3af66be077252b0.css" as="style"/><link rel="stylesheet" href="https://icetek.io/_next/static/css/f3af66be077252b0.css" data-n-g=""/><link rel="preload" href="https://icetek.io/_next/static/css/aa285a68ac3ec529.css" as="style"/><link rel="stylesheet" href="https://icetek.io/_next/static/css/aa285a68ac3ec529.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="https://icetek.io/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="https://icetek.io/_next/static/chunks/webpack-0812fca1e6499f68.js" defer=""></script><script src="https://icetek.io/_next/static/chunks/framework-3b5a00d5d7e8d93b.js" defer=""></script><script src="https://icetek.io/_next/static/chunks/main-d2a793b6dc23a82a.js" defer=""></script><script src="https://icetek.io/_next/static/chunks/pages/_app-db38936be70ff848.js" defer=""></script><script src="https://icetek.io/_next/static/chunks/839-ce0b23dc3526a602.js" defer=""></script><script src="https://icetek.io/_next/static/chunks/pages/blog/%5Bslug%5D-0f775c16935d7180.js" defer=""></script><script src="https://icetek.io/_next/static/k3szOxPXUEBMHu3BS-Ijr/_buildManifest.js" defer=""></script><script src="https://icetek.io/_next/static/k3szOxPXUEBMHu3BS-Ijr/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="navbar navbar-expand-lg navbar-light bg-white fixed-top page-header"><a class="navbar-brand" href="/"><picture><img src="/images/logo.jpg" width="auto" height="100%" alt="Home"/></picture></a><button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarContent" aria-controls="navbarContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"><svg stroke="currentColor" fill="currentColor" stroke-width="0" version="1.1" viewBox="0 0 17 17" class="show-menu-icon nav-icon" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><g></g><path d="M16 3v2h-15v-2h15zM1 10h15v-2h-15v2zM1 15h15v-2h-15v2z"></path></svg><svg stroke="currentColor" fill="currentColor" stroke-width="0" version="1.1" viewBox="0 0 17 17" class="hide-menu-icon nav-icon" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><g></g><path d="M9.207 8.5l6.646 6.646-0.707 0.707-6.646-6.646-6.646 6.646-0.707-0.707 6.646-6.646-6.647-6.646 0.707-0.707 6.647 6.646 6.646-6.646 0.707 0.707-6.646 6.646z"></path></svg></span></button><div id="navbarContent" class="collapse navbar-collapse"><ul class="navbar-nav mr-auto"><li class="nav-item "><a class="nav-link" href="/">Home</a></li><li class="nav-item "><a class="nav-link" href="/products">Products</a></li><li class="nav-item "><a class="nav-link" href="/services">Services</a></li><li class="nav-item "><a class="nav-link" href="/contact">Contact</a></li><li class="nav-item active"><a class="nav-link" href="/blog">Blog</a></li></ul></div></header><div class="container"><div class="row"><div class="col-lg-10 m-auto"><div class="card-page"><div class="my-10"><div class="row"><div class="col-2"><picture><img class="card-img-top rounded-circle" src="/blog-images/jakub-czaplinski.jpeg" alt="..."/></picture></div><div class="col align-self-center "><div class=" align-middle"><h6 class="small text-muted">Jakub Czapliński</h6><div class="small text-muted">14/5/2020</div></div></div></div></div><a href="/blog/how-to-connect-jetson-nano-to-kubernetes-using-k3s-and-k3sup"><picture><img class="card-img-top" src="/blog-images/jetson-header-img.jpeg" alt="..."/></picture></a><h1 class="card-title my-10">How To Connect Jetson Nano To Kubernetes Using K3s And K3sup</h1><div class="post-body my-10"><p>In this article, I will show how to connect a Jetson Nano Developer board to the Kubernetes cluster to act as a GPU node. I will cover the setup of NVIDIA docker needed to run containers using GPU and connecting Jetson to the Kubernetes cluster. After successfully connecting the node to the cluster I will also show how to run a simple TensorFlow 2 training session using the GPU on the Jetson Nano.
If you are interested in setting up a K3s cluster, you can follow my other tutorial explaining how to build a K3s cluster on Raspberry Pi using Ubuntu Server 18.04. Most of the information provided here is not unique to Raspberry Pi.</p>
<h2 id="k3s-or-kubernetes">K3s or Kubernetes?</h2>
<p>K3s is a lightweight version of Kubernetes that is optimized for smaller installations which, in my opinion, is ideal for single-board computers as it takes significantly fewer resources. You can read more about it <a href="https://k3s.io/">here</a>. On the other hand, K3sup is a great open-source tool built by Alex Ellis for simplifying the installation of K3s clusters. You can find more information about it on the <a href="https://github.com/alexellis/k3sup">https://github.com/alexellis/k3sup</a></p>
<h2 id="what-do-we-need">What do we need?</h2>
<ul>
<li>A K3s cluster — only a properly configured master node is required</li>
<li>NVIDIA Jetson Nano Developer board with the Developer Kit installed. For more information on how to install the developer kit on the board follow the instruction in the documentation found <a href="https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#write">https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#write</a></li>
<li>K3sup</li>
<li>15 minutes</li>
</ul>
<h2 id="plan">Plan</h2>
<ul>
<li>Setup NVIDIA docker</li>
<li>Add Jetson Nano to the K3s cluster</li>
<li>Run a simple MNIST example to showcase the usage of GPU inside the Kubernetes pod</li>
</ul>
<h2 id="setting-up-nvidia-docker">Setting up NVIDIA docker</h2>
<p>Before we configure Docker to use nvidia-docker as a default runtime, I would like to spend a moment explaining why this is needed. By default, when users run containers on Jetson Nano they will run in the same way as on any other hardware and you can’t access the GPU from the container, or at least not without some hacking. If you want to test it out by yourself you can run the following command and should see similar results</p>
<pre><code class="language-bash">root@jetson:~# echo &quot;python3 -c &#39;import tensorflow&#39;&quot; | docker run -i icetekio/jetson-nano-tensorflow /bin/bash
2020-05-14 00:10:23.370761: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library &#39;libcudart.so.10.2&#39;; dlerror: libcudart.so.10.2: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.2/targets/aarch64-linux/lib:
2020-05-14 00:10:23.370859: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2020-05-14 00:10:25.946896: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library &#39;libnvinfer.so.7&#39;; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.2/targets/aarch64-linux/lib:
2020-05-14 00:10:25.947219: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library &#39;libnvinfer_plugin.so.7&#39;; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.2/targets/aarch64-linux/lib:
2020-05-14 00:10:25.947273: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
</code></pre>
<p>If you now try to run the same command but add <code>--runtime=nvidia</code> parameter to the docker command you should see something like this</p>
<pre><code class="language-bash">root@jetson:~# echo &quot;python3 -c &#39;import tensorflow&#39;&quot; | docker run --runtime=nvidia -i icetekio/jetson-nano-tensorflow /bin/bash
2020-05-14 00:12:16.767624: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2020-05-14 00:12:19.386354: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libnvinfer.so.7
2020-05-14 00:12:19.388700: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libnvinfer_plugin.so.7
/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
</code></pre>
<p>The nvidia-docker is configured, however not enabled by default. To enable docker to run nvidia-docker runtime as a default — add the <code>&quot;default-runtime&quot;: &quot;nvidia&quot;</code> to the <code>/etc/docker/daemon.json</code> config file so it will look like this</p>
<pre><code class="language-jsx">{
    &quot;runtimes&quot;: {
        &quot;nvidia&quot;: {
            &quot;path&quot;: &quot;nvidia-container-runtime&quot;,
            &quot;runtimeArgs&quot;: []
        }
    },
    &quot;default-runtime&quot;: &quot;nvidia&quot;
}
</code></pre>
<p>Now you can skip the <code>--runtime=nvidia</code> argument in the docker run command, and the GPU will be initialized by default. This is needed so that K3s will use Docker with the nvidia-docker runtime allowing the pods to use GPU without any hassle and special configuration.</p>
<h2 id="connecting-jetson-as-a-kubernetes-node">Connecting Jetson as a Kubernetes node</h2>
<p>Connecting Jetson as a Kubernetes node using K3sup is only 1 command, however for it to work we need to be able to connect to both Jetson and the master node without a password and do <code>sudo</code> without a password, or to connect as a root user.</p>
<p>If you need to generate SSH keys and copy them over you can run something like this</p>
<pre><code class="language-bash">ssh-keygen -t rsa -b 4096 -f ~/.ssh/rpi -P &quot;&quot;
ssh-copy-id -i .ssh/rpi user@host
</code></pre>
<p>By default, Ubuntu installations require users to put in a password for <code>sudo</code> command. Because of that, the easier way is to use K3sup with a root account. To make this work copy your <code>~/.ssh/authorized_keys</code> to <code>/root/.ssh/ directory</code>.</p>
<p>Before connecting Jetson, let&#39;s look at the cluster we want to connect it to</p>
<pre><code class="language-bash">upgrade@ZeroOne:~$ kubectl get node -o wide
NAME      STATUS   ROLES    AGE   VERSION        INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
nexus     Ready    master   32d   v1.17.2+k3s1   192.168.0.12   &lt;none&gt;        Ubuntu 18.04.4 LTS   4.15.0-96-generic   containerd://1.3.3-k3s1
rpi3-32   Ready    &lt;none&gt;   32d   v1.17.2+k3s1   192.168.0.30   &lt;none&gt;        Ubuntu 18.04.4 LTS   5.3.0-1022-raspi2   containerd://1.3.3-k3s1
rpi3-64   Ready    &lt;none&gt;   32d   v1.17.2+k3s1   192.168.0.32   &lt;none&gt;        Ubuntu 18.04.4 LTS   5.3.0-1022-raspi2   containerd://1.3.3-k3s1
</code></pre>
<p>As you may notice, the master node is the <code>nexus</code> host on IP <code>192.168.0.12</code> that is running containerd. By default, k3s is running containerd but that can be modified. The containerd is a bit problematic as we set up the nvidia-docker to run with Docker and it is needed for the GPU. Fortunately, to switch from containerd to Docker we just need to pass one additional parameter to the k3sup command. So, finally, to connect our Jetson to the cluster we can run:</p>
<pre><code class="language-bash">k3sup join --ssh-key ~/.ssh/rpi  --server-ip 192.168.0.12  --ip 192.168.0.40   --k3s-extra-args &#39;--docker&#39;
</code></pre>
<p>The IP <code>192.168.0.40</code> is my Jetson Nano. As you can see we passed the <code>--k3s-extra-args &#39;--docker&#39;</code> flag that passes the <code>--docker</code> flag to the k3s agent while installing it. Thanks to that, we are using the docker with nvidia-docker setup rather than containerd.</p>
<p>To check if the node connected correctly we can run <code>kubectl get node -o wide</code></p>
<pre><code class="language-bash">upgrade@ZeroOne:~$ kubectl get node -o wide
NAME      STATUS   ROLES    AGE   VERSION        INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
nexus     Ready    master   32d   v1.17.2+k3s1   192.168.0.12   &lt;none&gt;        Ubuntu 18.04.4 LTS   4.15.0-96-generic   containerd://1.3.3-k3s1
rpi3-32   Ready    &lt;none&gt;   32d   v1.17.2+k3s1   192.168.0.30   &lt;none&gt;        Ubuntu 18.04.4 LTS   5.3.0-1022-raspi2   containerd://1.3.3-k3s1
rpi3-64   Ready    &lt;none&gt;   32d   v1.17.2+k3s1   192.168.0.32   &lt;none&gt;        Ubuntu 18.04.4 LTS   5.3.0-1022-raspi2   containerd://1.3.3-k3s1
jetson    Ready    &lt;none&gt;   11s   v1.17.2+k3s1   192.168.0.40   &lt;none&gt;        Ubuntu 18.04.4 LTS   4.9.140-tegra       docker://19.3.6
</code></pre>
<h2 id="simple-validation">Simple validation</h2>
<p>We can now run the pod using the same docker image and command to check if we will have the same results as running docker on Jetson Nano at the beginning of this article.</p>
<p>To do this, we can apply this pod spec:</p>
<pre><code class="language-bash">apiVersion: v1
kind: Pod
metadata:
  name: gpu-test
spec:
  nodeSelector:
    kubernetes.io/hostname: jetson
  containers:
  - image: icetekio/jetson-nano-tensorflow
    name: gpu-test
    command:
    - &quot;/bin/bash&quot;
    - &quot;-c&quot;
    - &quot;echo &#39;import tensorflow&#39; | python3&quot;
  restartPolicy: Never
</code></pre>
<p>Wait for the docker image to pull and then view the logs by running:</p>
<pre><code class="language-bash">upgrade@ZeroOne:~$ kubectl logs gpu-test
2020-05-14 10:01:51.341661: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
2020-05-14 10:01:53.996300: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libnvinfer.so.7
2020-05-14 10:01:53.998563: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libnvinfer_plugin.so.7
/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
</code></pre>
<p>As you can see, we have similar log messages as previously running docker on the Jetson!</p>
<h2 id="running-mnist-training">Running MNIST training</h2>
<p>We have a running node with GPU support, so now we can test out the “Hello world” of Machine Learning and run the TensorFlow 2 model example using MNIST dataset.</p>
<p>To run a simple training session that will prove the usage of GPU apply the manifest below.</p>
<pre><code class="language-bash">apiVersion: v1
kind: Pod
metadata:
  name: mnist-training
spec:
  nodeSelector:
    kubernetes.io/hostname: jetson
  initContainers:
    - name: git-clone
      image: iceci/utils
      command:
        - &quot;git&quot;
        - &quot;clone&quot;
        - &quot;https://github.com/IceCI/example-mnist-training.git&quot;
        - &quot;/workspace&quot;
      volumeMounts:
        - mountPath: /workspace
          name: workspace
  containers:
    - image: icetekio/jetson-nano-tensorflow
      name: mnist
      command:
        - &quot;python3&quot;
        - &quot;/workspace/mnist.py&quot;
      volumeMounts:
        - mountPath: /workspace
          name: workspace
  restartPolicy: Never
  volumes:
    - name: workspace
      emptyDir: {}
</code></pre>
<p>As you can see in the log below, the GPU is running.</p>
<pre><code class="language-bash">...
2020-05-14 11:30:02.846289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-05-14 11:30:02.846434: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2
....
</code></pre>
<p>If you are on the node, you can test the usage of CPU and GPU by running <code>tegrastats</code> command</p>
<pre><code class="language-bash">upgrade@jetson:~$ tegrastats --interval 5000
RAM 2462/3964MB (lfb 2x4MB) SWAP 362/1982MB (cached 6MB) CPU [52%@1479,41%@1479,43%@1479,34%@1479] EMC_FREQ 0% GR3D_FREQ 9% PLL@23.5C CPU@26C PMIC@100C GPU@24C AO@28.5C thermal@25C POM_5V_IN 3410/3410 POM_5V_GPU 451/451 POM_5V_CPU 1355/1355
RAM 2462/3964MB (lfb 2x4MB) SWAP 362/1982MB (cached 6MB) CPU [53%@1479,42%@1479,45%@1479,35%@1479] EMC_FREQ 0% GR3D_FREQ 9% PLL@23.5C CPU@26C PMIC@100C GPU@24C AO@28.5C thermal@24.75C POM_5V_IN 3410/3410 POM_5V_GPU 451/451 POM_5V_CPU 1353/1354
RAM 2461/3964MB (lfb 2x4MB) SWAP 362/1982MB (cached 6MB) CPU [52%@1479,38%@1479,43%@1479,33%@1479] EMC_FREQ 0% GR3D_FREQ 10% PLL@24C CPU@26C PMIC@100C GPU@24C AO@29C thermal@25.25C POM_5V_IN 3410/3410 POM_5V_GPU 493/465 POM_5V_CPU 1314/1340
</code></pre>
<h2 id="summary">Summary</h2>
<p>As you can see, hooking up a Jetson Nano to a Kubernetes cluster is a pretty simple and straightforward process. In just a couple of minutes, you’ll be able to leverage Kubernetes to run machine learning workloads as well as use the power of NVIDIA’s pocket-sized GPU. You’ll be able to run any GPU containers designed for Jetson Nano on Kubernetes, which can simplify your development and testing.</p>
<h2 id="readout">Readout</h2>
<ul>
<li>Jetson Nano Developer Kit documentation: <a href="https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#intro">https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#intro</a></li>
<li>NVIDIA docker repository including the overview of NVIDIA docker: <a href="https://github.com/NVIDIA/nvidia-docker">https://github.com/NVIDIA/nvidia-docker</a></li>
<li>K3s website: <a href="https://k3s.io/">https://k3s.io/</a></li>
<li>K3sup: <a href="https://github.com/alexellis/k3sup">https://github.com/alexellis/k3sup</a></li>
<li>MNIST model code used from TensorFlow 2 documentation: <a href="https://www.tensorflow.org/overview">https://www.tensorflow.org/overview</a></li>
</ul>
</div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"data":{"author":"Jakub Czapliński","authorPhoto":"jakub-czaplinski.jpeg","date":"2020-05-14T07:50:39Z","image":"jetson-header-img.jpeg","slug":"how-to-connect-jetson-nano-to-kubernetes-using-k3s-and-k3sup","summary":"In this article I will show how to connect a Jetson Nano Developer board to the Kubernetes cluster to act as a GPU node.","title":"How To Connect Jetson Nano To Kubernetes Using K3s And K3sup"},"slug":"how-to-connect-jetson-nano-to-kubernetes-using-k3s-and-k3sup","content":"\nIn this article, I will show how to connect a Jetson Nano Developer board to the Kubernetes cluster to act as a GPU node. I will cover the setup of NVIDIA docker needed to run containers using GPU and connecting Jetson to the Kubernetes cluster. After successfully connecting the node to the cluster I will also show how to run a simple TensorFlow 2 training session using the GPU on the Jetson Nano.\nIf you are interested in setting up a K3s cluster, you can follow my other tutorial explaining how to build a K3s cluster on Raspberry Pi using Ubuntu Server 18.04. Most of the information provided here is not unique to Raspberry Pi.\n\n## K3s or Kubernetes?\n\nK3s is a lightweight version of Kubernetes that is optimized for smaller installations which, in my opinion, is ideal for single-board computers as it takes significantly fewer resources. You can read more about it [here](https://k3s.io/). On the other hand, K3sup is a great open-source tool built by Alex Ellis for simplifying the installation of K3s clusters. You can find more information about it on the [https://github.com/alexellis/k3sup](https://github.com/alexellis/k3sup)\n\n## What do we need?\n\n- A K3s cluster — only a properly configured master node is required\n- NVIDIA Jetson Nano Developer board with the Developer Kit installed. For more information on how to install the developer kit on the board follow the instruction in the documentation found [https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#write](https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#write)\n- K3sup\n- 15 minutes\n\n## Plan\n\n- Setup NVIDIA docker\n- Add Jetson Nano to the K3s cluster\n- Run a simple MNIST example to showcase the usage of GPU inside the Kubernetes pod\n\n## Setting up NVIDIA docker\n\nBefore we configure Docker to use nvidia-docker as a default runtime, I would like to spend a moment explaining why this is needed. By default, when users run containers on Jetson Nano they will run in the same way as on any other hardware and you can’t access the GPU from the container, or at least not without some hacking. If you want to test it out by yourself you can run the following command and should see similar results\n\n```bash\nroot@jetson:~# echo \"python3 -c 'import tensorflow'\" | docker run -i icetekio/jetson-nano-tensorflow /bin/bash\n2020-05-14 00:10:23.370761: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.2'; dlerror: libcudart.so.10.2: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.2/targets/aarch64-linux/lib:\n2020-05-14 00:10:23.370859: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2020-05-14 00:10:25.946896: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.2/targets/aarch64-linux/lib:\n2020-05-14 00:10:25.947219: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.2/targets/aarch64-linux/lib:\n2020-05-14 00:10:25.947273: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n```\n\nIf you now try to run the same command but add `--runtime=nvidia` parameter to the docker command you should see something like this\n\n```bash\nroot@jetson:~# echo \"python3 -c 'import tensorflow'\" | docker run --runtime=nvidia -i icetekio/jetson-nano-tensorflow /bin/bash\n2020-05-14 00:12:16.767624: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\n2020-05-14 00:12:19.386354: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libnvinfer.so.7\n2020-05-14 00:12:19.388700: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libnvinfer_plugin.so.7\n/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n```\n\nThe nvidia-docker is configured, however not enabled by default. To enable docker to run nvidia-docker runtime as a default — add the `\"default-runtime\": \"nvidia\"` to the `/etc/docker/daemon.json` config file so it will look like this\n\n```jsx\n{\n    \"runtimes\": {\n        \"nvidia\": {\n            \"path\": \"nvidia-container-runtime\",\n            \"runtimeArgs\": []\n        }\n    },\n    \"default-runtime\": \"nvidia\"\n}\n```\n\nNow you can skip the `--runtime=nvidia` argument in the docker run command, and the GPU will be initialized by default. This is needed so that K3s will use Docker with the nvidia-docker runtime allowing the pods to use GPU without any hassle and special configuration.\n\n## Connecting Jetson as a Kubernetes node\n\nConnecting Jetson as a Kubernetes node using K3sup is only 1 command, however for it to work we need to be able to connect to both Jetson and the master node without a password and do `sudo` without a password, or to connect as a root user.\n\nIf you need to generate SSH keys and copy them over you can run something like this\n\n```bash\nssh-keygen -t rsa -b 4096 -f ~/.ssh/rpi -P \"\"\nssh-copy-id -i .ssh/rpi user@host\n```\n\nBy default, Ubuntu installations require users to put in a password for `sudo` command. Because of that, the easier way is to use K3sup with a root account. To make this work copy your `~/.ssh/authorized_keys` to `/root/.ssh/ directory`.\n\nBefore connecting Jetson, let's look at the cluster we want to connect it to\n\n```bash\nupgrade@ZeroOne:~$ kubectl get node -o wide\nNAME      STATUS   ROLES    AGE   VERSION        INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nnexus     Ready    master   32d   v1.17.2+k3s1   192.168.0.12   \u003cnone\u003e        Ubuntu 18.04.4 LTS   4.15.0-96-generic   containerd://1.3.3-k3s1\nrpi3-32   Ready    \u003cnone\u003e   32d   v1.17.2+k3s1   192.168.0.30   \u003cnone\u003e        Ubuntu 18.04.4 LTS   5.3.0-1022-raspi2   containerd://1.3.3-k3s1\nrpi3-64   Ready    \u003cnone\u003e   32d   v1.17.2+k3s1   192.168.0.32   \u003cnone\u003e        Ubuntu 18.04.4 LTS   5.3.0-1022-raspi2   containerd://1.3.3-k3s1\n```\n\nAs you may notice, the master node is the `nexus` host on IP `192.168.0.12` that is running containerd. By default, k3s is running containerd but that can be modified. The containerd is a bit problematic as we set up the nvidia-docker to run with Docker and it is needed for the GPU. Fortunately, to switch from containerd to Docker we just need to pass one additional parameter to the k3sup command. So, finally, to connect our Jetson to the cluster we can run:\n\n```bash\nk3sup join --ssh-key ~/.ssh/rpi  --server-ip 192.168.0.12  --ip 192.168.0.40   --k3s-extra-args '--docker'\n```\n\nThe IP `192.168.0.40` is my Jetson Nano. As you can see we passed the `--k3s-extra-args '--docker'` flag that passes the `--docker` flag to the k3s agent while installing it. Thanks to that, we are using the docker with nvidia-docker setup rather than containerd.\n\nTo check if the node connected correctly we can run `kubectl get node -o wide`\n\n```bash\nupgrade@ZeroOne:~$ kubectl get node -o wide\nNAME      STATUS   ROLES    AGE   VERSION        INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nnexus     Ready    master   32d   v1.17.2+k3s1   192.168.0.12   \u003cnone\u003e        Ubuntu 18.04.4 LTS   4.15.0-96-generic   containerd://1.3.3-k3s1\nrpi3-32   Ready    \u003cnone\u003e   32d   v1.17.2+k3s1   192.168.0.30   \u003cnone\u003e        Ubuntu 18.04.4 LTS   5.3.0-1022-raspi2   containerd://1.3.3-k3s1\nrpi3-64   Ready    \u003cnone\u003e   32d   v1.17.2+k3s1   192.168.0.32   \u003cnone\u003e        Ubuntu 18.04.4 LTS   5.3.0-1022-raspi2   containerd://1.3.3-k3s1\njetson    Ready    \u003cnone\u003e   11s   v1.17.2+k3s1   192.168.0.40   \u003cnone\u003e        Ubuntu 18.04.4 LTS   4.9.140-tegra       docker://19.3.6\n```\n\n## Simple validation\n\nWe can now run the pod using the same docker image and command to check if we will have the same results as running docker on Jetson Nano at the beginning of this article.\n\nTo do this, we can apply this pod spec:\n\n```bash\napiVersion: v1\nkind: Pod\nmetadata:\n  name: gpu-test\nspec:\n  nodeSelector:\n    kubernetes.io/hostname: jetson\n  containers:\n  - image: icetekio/jetson-nano-tensorflow\n    name: gpu-test\n    command:\n    - \"/bin/bash\"\n    - \"-c\"\n    - \"echo 'import tensorflow' | python3\"\n  restartPolicy: Never\n```\n\nWait for the docker image to pull and then view the logs by running:\n\n```bash\nupgrade@ZeroOne:~$ kubectl logs gpu-test\n2020-05-14 10:01:51.341661: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\n2020-05-14 10:01:53.996300: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libnvinfer.so.7\n2020-05-14 10:01:53.998563: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libnvinfer_plugin.so.7\n/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n```\n\nAs you can see, we have similar log messages as previously running docker on the Jetson!\n\n## Running MNIST training\n\nWe have a running node with GPU support, so now we can test out the “Hello world” of Machine Learning and run the TensorFlow 2 model example using MNIST dataset.\n\nTo run a simple training session that will prove the usage of GPU apply the manifest below.\n\n```bash\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mnist-training\nspec:\n  nodeSelector:\n    kubernetes.io/hostname: jetson\n  initContainers:\n    - name: git-clone\n      image: iceci/utils\n      command:\n        - \"git\"\n        - \"clone\"\n        - \"https://github.com/IceCI/example-mnist-training.git\"\n        - \"/workspace\"\n      volumeMounts:\n        - mountPath: /workspace\n          name: workspace\n  containers:\n    - image: icetekio/jetson-nano-tensorflow\n      name: mnist\n      command:\n        - \"python3\"\n        - \"/workspace/mnist.py\"\n      volumeMounts:\n        - mountPath: /workspace\n          name: workspace\n  restartPolicy: Never\n  volumes:\n    - name: workspace\n      emptyDir: {}\n```\n\nAs you can see in the log below, the GPU is running.\n\n```bash\n...\n2020-05-14 11:30:02.846289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n2020-05-14 11:30:02.846434: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\n....\n```\n\nIf you are on the node, you can test the usage of CPU and GPU by running `tegrastats` command\n\n```bash\nupgrade@jetson:~$ tegrastats --interval 5000\nRAM 2462/3964MB (lfb 2x4MB) SWAP 362/1982MB (cached 6MB) CPU [52%@1479,41%@1479,43%@1479,34%@1479] EMC_FREQ 0% GR3D_FREQ 9% PLL@23.5C CPU@26C PMIC@100C GPU@24C AO@28.5C thermal@25C POM_5V_IN 3410/3410 POM_5V_GPU 451/451 POM_5V_CPU 1355/1355\nRAM 2462/3964MB (lfb 2x4MB) SWAP 362/1982MB (cached 6MB) CPU [53%@1479,42%@1479,45%@1479,35%@1479] EMC_FREQ 0% GR3D_FREQ 9% PLL@23.5C CPU@26C PMIC@100C GPU@24C AO@28.5C thermal@24.75C POM_5V_IN 3410/3410 POM_5V_GPU 451/451 POM_5V_CPU 1353/1354\nRAM 2461/3964MB (lfb 2x4MB) SWAP 362/1982MB (cached 6MB) CPU [52%@1479,38%@1479,43%@1479,33%@1479] EMC_FREQ 0% GR3D_FREQ 10% PLL@24C CPU@26C PMIC@100C GPU@24C AO@29C thermal@25.25C POM_5V_IN 3410/3410 POM_5V_GPU 493/465 POM_5V_CPU 1314/1340\n```\n\n## Summary\n\nAs you can see, hooking up a Jetson Nano to a Kubernetes cluster is a pretty simple and straightforward process. In just a couple of minutes, you’ll be able to leverage Kubernetes to run machine learning workloads as well as use the power of NVIDIA’s pocket-sized GPU. You’ll be able to run any GPU containers designed for Jetson Nano on Kubernetes, which can simplify your development and testing.\n\n## Readout\n\n- Jetson Nano Developer Kit documentation: [https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#intro](https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#intro)\n- NVIDIA docker repository including the overview of NVIDIA docker: [https://github.com/NVIDIA/nvidia-docker](https://github.com/NVIDIA/nvidia-docker)\n- K3s website: [https://k3s.io/](https://k3s.io/)\n- K3sup: [https://github.com/alexellis/k3sup](https://github.com/alexellis/k3sup)\n- MNIST model code used from TensorFlow 2 documentation: [https://www.tensorflow.org/overview](https://www.tensorflow.org/overview)\n"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"how-to-connect-jetson-nano-to-kubernetes-using-k3s-and-k3sup"},"buildId":"k3szOxPXUEBMHu3BS-Ijr","assetPrefix":"https://icetek.io","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body><script defer="" src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script><script defer="" src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script><script defer="" src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script></html></body></html>